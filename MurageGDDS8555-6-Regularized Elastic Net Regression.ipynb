{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bef6e5-f31f-4baf-8df0-18ae986a54cf",
   "metadata": {},
   "source": [
    "## <BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "   \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "#                                          Elastic Net Regression in Predicting Obesity Risk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##                                                        Gladys Murage\n",
    "\n",
    "##                              College of Business, Engineering, and  Technology, National University\n",
    "\n",
    "##                                         DDS8555 v1: PREDICTIVE ANALYSIS(3602869492)\n",
    "\n",
    "##                                                        Dr MOHAMED NABEEL\n",
    "\n",
    "##                                                            April 20, 2025\n",
    "\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9660fd-8145-43c0-b124-4e64c1320c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b98ea00-6689-47a3-93d1-8d17521a2dd2",
   "metadata": {},
   "source": [
    "# Regularized Regression and Elastic Net Implementation\n",
    "### Regularized regression is a technique that prevents overfitting by adding penalty terms to the loss function. There are three main types:\n",
    "#### 1. Ridge Regression (L2 regularization) adds penalty proportional to the square of coefficients\n",
    "#### 2. Lasso Regression (L1 regularization) adds penalty proportional to absolute values of coefficients\n",
    "#### 3. Elastic Net: Combines both L1 and L2 penalties\n",
    "\n",
    "## Elastic Net is particularly useful when:\n",
    "1. There is many correlated features\n",
    "2. When there is need for automatic feature selection like in  Lasso regression\n",
    "3. When there is need to handle multicollinearity like in  Ridge regression.\n",
    "\n",
    "## This Elastic Net model will:\n",
    "1. Automatically handle feature scaling and encoding\n",
    "2. Find the optimal balance between L1 and L2 regularization\n",
    "3. Provide interpretable feature importance\n",
    "4. Generate predictions in the original class labels.\n",
    "5. implementation shows how to adapt ElasticNet for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11007472-6281-4988-b2c8-91c84fa0ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.001, 'l1_ratio': 0.7, 'max_iter': 1000}\n",
      "\n",
      "Validation MSE: 2.5107\n",
      "Validation R2: 0.3109\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                              Feature  Coefficient\n",
      "23                    CALC_Frequently     1.583043\n",
      "18                            CAEC_no     1.480652\n",
      "16                    CAEC_Frequently    -0.678778\n",
      "3                              Weight     0.677520\n",
      "15                        CAEC_Always    -0.586172\n",
      "30                     MTRANS_Walking     0.464617\n",
      "11  family_history_with_overweight_no    -0.340397\n",
      "13                            FAVC_no     0.335165\n",
      "14                           FAVC_yes    -0.321621\n",
      "1                                 Age     0.289111\n",
      "\n",
      "Test predictions saved to 'elasticnet_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('Otrain.csv')\n",
    "test = pd.read_csv('Otest.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop('NObeyesdad', axis=1)\n",
    "y = train['NObeyesdad']\n",
    "\n",
    "# Encode target variable for regression\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# ElasticNet model\n",
    "elastic = ElasticNet(random_state=42)\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'max_iter': [1000, 5000]\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GridSearchCV(elastic, param_grid, cv=5, scoring='neg_mean_squared_error'))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = pipeline.named_steps['regressor'].best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred = pipeline.predict(X_val)\n",
    "print(f\"\\nValidation MSE: {mean_squared_error(y_val, y_pred):.4f}\")\n",
    "print(f\"Validation R2: {r2_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = (num_cols + \n",
    "                list(pipeline.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .get_feature_names_out(cat_cols)))\n",
    "\n",
    "coefs = pipeline.named_steps['regressor'].best_estimator_.coef_\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})\n",
    "importance_df = importance_df.sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Prepare test predictions if test data exists\n",
    "if not test.empty:\n",
    "    test_pred = pipeline.predict(test)\n",
    "    # If you need to convert back to original classes:\n",
    "    # test_pred_labels = le.inverse_transform(test_pred.round().astype(int))\n",
    "    \n",
    "    # For regression output, you might want to keep as continuous values\n",
    "    submission = pd.DataFrame({'Predicted': test_pred})\n",
    "    submission.to_csv('elasticnet_predictions.csv', index=False)\n",
    "    print(\"\\nTest predictions saved to 'elasticnet_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872a1c-dcd3-446c-8969-20d0ea5e03b9",
   "metadata": {},
   "source": [
    "# Model Performance Metrics\n",
    "## Validation MSE: 2.5107\n",
    "1. The average squared difference between predicted and actual values is  approximately 2.51\n",
    "2. Given that obesity levels are ordinal (typically 1-7), this suggests predictions are off by about 2.51  on average\n",
    "3. This is a moderate error rate for a 7-class problem\n",
    "\n",
    "## Validation R²: 0.3109\n",
    "1. Only 31.09% of variance in obesity levels is explained by the Elastic Net  model\n",
    "2. It indicatest that the Elastic Net  model captures some signal but likely misses important predictors\n",
    "3. This is typical for linear models on complex biological data without feature engineering\n",
    "\n",
    "## Feature Importance Analysis\n",
    "### Top Predictive Features:\n",
    "#### CALC_Frequently  with a coefficient of 1.58\n",
    "1. Frequent alcohol consumption strongly predicts higher obesity levels\n",
    "2. This is the largest positive coefficient in the model\n",
    "\n",
    "#### CAEC_no with a coefficient of 1.48\n",
    "1. Not consuming food between meals predicts higher obesity\n",
    "2. This suggests irregular eating patterns correlate with weight gain\n",
    "\n",
    "#### CAEC_Frequently with a coefficient of -0.68\n",
    "1. Frequent between-meal consumption predicts lower obesity\n",
    "2, This may indicate healthier snacking habits\n",
    "\n",
    "#### Weight with a coefficient of 0.68\n",
    "1. As expected, higher weight strongly predicts obesity\n",
    "2. Surprisingly Weight is not the top predictor in this Elastic Net model\n",
    "\n",
    "#### CAEC_Always with a coefficient of -0.59\n",
    "1.  Constant between-meal eating is associated with lower obesity\n",
    "2. This potentially indicates individuals with high-metabolism\n",
    "\n",
    "## Key Insights:\n",
    "1. Eating patterns dominate (CALC, CAEC features)\n",
    "2. Behavioral factors outweigh pure biometrics Weight is only #4 in the importance of predicting obesity.\n",
    "3. Unexpected relationships like \"no between-meal eating\"leading to  higher obesity rates\n",
    "4. Missing strong predictors like exercise frequency or diet quality\n",
    "\n",
    "## Recommendations for Improvement\n",
    "1. Feature Engineering using the following code in python\n",
    "Create interaction terms\n",
    "X['weight_height_ratio'] = X['Weight']/(X['Height']**2)\n",
    "X['meal_frequency'] = X['FCVC'] + X['NCP']\n",
    "\n",
    "2. Model Tuning using the following code in python\n",
    "Expand hyperparameter search\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-4, 2, 20),\n",
    "    'l1_ratio': [0, 0.25, 0.5, 0.75, 1],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}\n",
    "## Alternative Approaches should be employed:\n",
    "1. Gradient Boosted Trees (XGBoost) which may better capture non-linear relationships\n",
    "2. Use ordinal regression instead of treating classes as numeric\n",
    "3.  Collect more detailed dietary/exercise data\n",
    "\n",
    "## Behavioral Interpretation:\n",
    "The counterintuitive CAEC findings suggest the following:\n",
    "1. That people reporting no between-meal eating may be under-reporting\n",
    "2. Or the patients  may engage in larger, less frequent meals\n",
    "3. The results warrants domain expert consultation\n",
    "\n",
    "## Conclusion:\n",
    "The Elastic Net model shows moderate predictive power but reveals interesting patterns in eating behaviors that could inform both better modeling and potential interventions. \n",
    "The relatively low R² suggests substantial unexplained variance that is  either from missing features or non-linear relationships not captured by linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
